---
title: "Logistic Regression - Week 13, Day 2"
author: "Hector Gavilanes"
format: html
self-contained: true
editor: source
---

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
library(gsheet)
library(gmodels)
library(ggplot2)
library(InformationValue) # miscalculation error
library(ISLR)
library(caret)
library(e1071)
library(cvms) # plot confusion matrix
# install.packages("ggimage")
# install.packages("rsvg")
```

```{r}
# clear environment
rm(list = ls())
# load data
data <- as_tibble(gsheet2tbl("https://docs.google.com/spreadsheets/d/1TrJ6jZ3hP6gQk6FvqbMrFUysEUEqle7XfuLnAkZsHYI/edit"))
```
**1. Challenge! Research "good" values of the metrics we discussed in lecture. How can we determine how well the model is performing using these metrics?**
There are different metrics to assess the quality of a model.

- Accuracy: The percentage of all observations that are correctly classified by the model.
- Sensitivity: The true positive rate; the probability of a positive test, conditioned on truly being positive.
- Specificity: The true negative rate; the probability of a negative test, conditioned on truly being negative.
- Total misclassification rate: The percentage of total incorrect classifications made by the model. The lower the value, the better.

There is no universal threshold to determine a "good" model. A baseline model which includes every observation in a dataset to belong to the most common class. A model with higher metrics than the baseline can be consider useful. The greater the difference in metrics, the better.  


**2. Recall question 1 from the last activity.**

**2a. Restate the model from 1a here.** 

```{r, echo = TRUE}
m1 <- glm(data$Survived ~ data$Age + data$SibSp + data$Parch + data$Pclass + data$Fare,
          family = binomial)
summary(m1)

# get coefficients
m1_coeff <- coefficients(m1)
```

$$
\ln \left( \frac{\hat{\pi}}{1-\hat{\pi}} \right) = `r round(m1_coeff[1],3)` `r round(m1_coeff[2],3)` \text{ Age} `r round(m1_coeff[3],3)` \text{ Sibling \& Spouse} + `r round(m1_coeff[4],3)` \text{ Parents \& Children} `r round(m1_coeff[5],3)` \text{ Class} + `r round(m1_coeff[6],3)` \text{ Fare}
$$

**2b. Find the confusion matrix for this model.**

```{r, echo = TRUE}
# exclude missing values
m1 <- glm(data$Survived ~ data$Age + data$SibSp + data$Parch + data$Pclass + data$Fare,
          family = binomial,
          na.action = na.exclude)

# get predicted values
data <- data %>% 
  mutate(p_hat = predict(m1, type = "response"))

# misclassification error
data <- data %>% 
  mutate(Predicted = ifelse(p_hat > 0.5, 1, 0))

# get Confusion Matrix
CrossTable(data$Predicted, data$Survived,
           prop.r = FALSE,
           prop.c = FALSE,
           prop.t = FALSE,
           prop.chisq = FALSE)

# convert to factor
Actual = factor(data$Survived)
Predicted = factor(data$Predicted)

# get confusion matrix formulas
confusionMatrix(Predicted, Actual, positive = "1")

```



**2c. Find the sensitivity of the model.**

```{r, echo = TRUE}
tp = 150
tn = 351
fn = 140
fp = 73

# get sensitivity (recall)
sensitivity = tp / (tp + fn)
round(sensitivity, 3)

# get miscalculation error
misClassError(data$Survived, data$Predicted)
1 - 0.2391
```

**2d. Find the specificity of the model.**

```{r, echo = TRUE}
# get specificity
specificity = tn / (tn + fp)
round(specificity, 3)
```

**2e. Find the positive predictive value of the model.**

```{r, echo = TRUE}
# get positive predictive value (Precision)
pos_pred_value = tp / (tp + fp)
round(pos_pred_value, 3)
```

**2f. Find the negative predictive value of the model.**

```{r, echo = TRUE}
# get negative predictive value
neg_pred_value = tn / (tn + fn)
round(neg_pred_value, 3)
```

**2g. Find the false discovery rate of the model.**

```{r, echo = TRUE}
# get false discovery rate
false_discovery_rate = fp / (fp + tp)
round(false_discovery_rate, 3)
```

**3. Compare and contrasts the results from the models constructed in  lecture and the last activity. Which model performs better, based on your answer in question 1?**

- The model from Q2 can be considered as the baseline model because it has the most common variables of the dataset.
- The model shown in lecture had less variables; thus, the model's metrics are slighty better than the baseline. 

**4. Challenge! Construct a graph to support your answer in question 3.**

```{r, echo = TRUE}
# plot confusion matrix
conf_matrix <- tibble("Predicted" = c(0, 0, 1, 1),
                      "Actual" =    c(0, 1, 0, 1),
                      "Counts" =    c(351, 140, 73, 150))

plot_confusion_matrix(conf_matrix,
                      target_col = "Actual",
                      prediction_col = "Predicted",
                      counts_col = "Counts")

```



