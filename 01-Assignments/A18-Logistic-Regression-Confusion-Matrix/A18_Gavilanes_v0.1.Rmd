---
title: "Logistic Regression - Week 13, Day 2"
author: "Hector Gavilanes"
format: html
self-contained: true
editor: source
---

```{r, echo=TRUE, warning=FALSE, message=FALSE}
library(tidyverse)
library(gsheet)
library(gmodels)
library(ggplot2)
library(InformationValue) # miscalculation error
library(ISLR)
library(caret)
library(e1071)
library(cvms) # plot confusion matrix

```

```{r}
# clear environment
rm(list = ls())
# load data
data <- as_tibble(gsheet2tbl("https://docs.google.com/spreadsheets/d/1TrJ6jZ3hP6gQk6FvqbMrFUysEUEqle7XfuLnAkZsHYI/edit"))
```
**1. Challenge! Research "good" values of the metrics we discussed in lecture. How can we determine how well the model is performing using these metrics?**
There are different metrics to assess the quality of a model.

- Accuracy: The percentage of all observations that are correctly classified by the model.
- Sensitivity: The true positive rate; the probability of a positive test, conditioned on truly being positive.
- Specificity: The true negative rate; the probability of a negative test, conditioned on truly being negative.
- Total misclassification rate: The percentage of total incorrect classifications made by the model. The lower the value, the better.

There is no universal threshold to determine a "good" model. A baseline model which includes every observation in a dataset to belong to the most common class. A model with higher metrics than the baseline can be consider useful. The greater the difference in metrics, the better.  


**2. Recall question 1 from the last activity.**

**2a. Restate the model from 1a here.** 

## The Baseline model
```{r, echo = TRUE}
base_mod <- glm(data$Survived ~ data$Age + data$SibSp + data$Parch + data$Pclass + data$Fare,
          family = binomial,
          na.action = na.exclude)
summary(base_mod)

# get coefficients
base_coeff <- coefficients(base_mod)
```

$$
\ln \left( \frac{\hat{\pi}}{1-\hat{\pi}} \right) = `r round(base_coeff[1],3)` `r round(base_coeff[2],3)` \text{ Age} `r round(base_coeff[3],3)` \text{ Sibling \& Spouse} + `r round(base_coeff[4],3)` \text{ Parents \& Children} `r round(base_coeff[5],3)` \text{ Class} + `r round(base_coeff[6],3)` \text{ Fare}
$$
- Age (p < 0.001), Siblings & Spouses (p = 0.006), Parents & Children (p = 0.023), and Passenger Class (p < 0.001) are significant predictors of survival.
- Fare (0.194) is not a significant predictor of survival. 

**2b. Find the confusion matrix for this model.**

```{r, echo = TRUE}
# get predicted values
data <- data %>% 
  mutate(p_hat = predict(base_mod, type = "response"))

# misclassification error
data <- data %>% 
  mutate(Predicted = ifelse(p_hat > 0.5, 1, 0))

# get Confusion Matrix cross-table
CrossTable(data$Predicted, data$Survived,
           prop.r = FALSE,
           prop.c = FALSE,
           prop.t = FALSE,
           prop.chisq = FALSE)

# convert to factor
Actual = factor(data$Survived)
Predicted = factor(data$Predicted)

# get confusion matrix function
confusionMatrix(Predicted, Actual, positive = "1")

```

**2c. Find the sensitivity of the model.**

```{r, echo = TRUE}
tp = 150
tn = 351
fn = 140
fp = 73

# get sensitivity (recall)
sensitivity = tp / (tp + fn)
round(sensitivity, 3)

# get miscalculation error
misClassError(data$Survived, data$Predicted)
1 - 0.2391
```

**2d. Find the specificity of the model.**

```{r, echo = TRUE}
# get specificity
specificity = tn / (tn + fp)
round(specificity, 3)
```

**2e. Find the positive predictive value of the model.**

```{r, echo = TRUE}
# get positive predictive value (Precision)
pos_pred_value = tp / (tp + fp)
round(pos_pred_value, 3)
```

**2f. Find the negative predictive value of the model.**

```{r, echo = TRUE}
# get negative predictive value
neg_pred_value = tn / (tn + fn)
round(neg_pred_value, 3)
```

**2g. Find the false discovery rate of the model.**

```{r, echo = TRUE}
# get false discovery rate
false_discovery_rate = fp / (fp + tp)
round(false_discovery_rate, 3)
```

**3. Compare and contrasts the results from the models constructed in  lecture and the last activity. Which model performs better, based on your answer in question 1?**

- The model from Q2 can be considered as the baseline model because it has the most common variables of the dataset.
- The model shown in lecture had less variables; however, the model's metrics are very similar to the baseline. 
- For both models, the Fare variable is not a significant predictor of survival.
- Perhaps, it is possible to obtain a better model without the Fare variable.

## Conclusion
- After conducting a Chi-square comparison between both models are equal informative. 
- Thus, the simpler model in this case will be suitable which is the one from the lecture.

```{r}
# drop Cabin, Passenger-ID, Name, Ticket columns
clean_data <- subset(data,select=c(2,3,6,7,8,10))

# replace missing values with mean for Age
clean_data$Age[is.na(clean_data$Age)] <- mean(clean_data$Age,na.rm=T)

# baseline model
m2 <- glm(clean_data$Survived ~ clean_data$Age + clean_data$SibSp + clean_data$Parch + clean_data$Pclass + clean_data$Fare,
          family = binomial,
          na.action = na.exclude)

# lecture model
m3 <- glm(clean_data$Survived ~ clean_data$Age + clean_data$Pclass + clean_data$Fare,
          family = binomial,
          na.action = na.exclude)
# models comparison
anova(object = m2, m3,
      test = "Chisq")

```



**4. Challenge! Construct a graph to support your answer in question 3.**

## Baseline Model Confusion Matrix
```{r, echo = TRUE}
# plot confusion matrix
base_matrix <- tibble("Predicted" = c(0, 0, 1, 1),
                      "Actual" =    c(0, 1, 0, 1),
                      "Counts" =    c(351, 140, 73, 150))

plot_confusion_matrix(base_matrix,
                      target_col = "Actual",
                      prediction_col = "Predicted",
                      counts_col = "Counts")

```

## Lecture Model Confusion Matrix
```{r}
# exclude missing values from the model
lecture_cf <- glm(data$Survived ~ data$Age + data$Pclass,
          family = binomial,
          na.action = na.exclude)

# get predicted values
lecture_data <- data %>% 
  mutate(p_hat = predict(lecture_cf, type = "response"))

# misclassification error
lecture_data <- data %>% 
  mutate(Predicted = ifelse(p_hat > 0.5, 1, 0))

# get Confusion Matrix cross-table
CrossTable(lecture_data$Predicted, lecture_data$Survived,
           prop.r = FALSE,
           prop.c = FALSE,
           prop.t = FALSE,
           prop.chisq = FALSE)

# convert to factor
Actual_lect = factor(lecture_data$Survived)
Predicted_lect = factor(lecture_data$Predicted)

# get confusion matrix function
confusionMatrix(Predicted_lect, Actual_lect, positive = "1")
```
```{r}
# plot confusion matrix from Lecture
base_matrix <- tibble("Predicted" = c(0, 0, 1, 1),
                      "Actual" =    c(0, 1, 0, 1),
                      "Counts" =    c(355, 140, 69, 150))

plot_confusion_matrix(base_matrix,
                      target_col = "Actual",
                      prediction_col = "Predicted",
                      counts_col = "Counts")
```
### Baseline Model Plot for all Predictors
```{r}

pairs(
  x = data[, c("Survived", "Age", "SibSp", "Parch", "Pclass", "Fare")],
  col = data$Survived + 5)

```

```{r, warning=FALSE}
library(car)
library(carData)

# baseline plot
car::mmps(model = m2,
          col = clean_data$Survived + 5)

```
## Lecture Model
```{r, warning=FALSE}
# lecture plot
car::mmps(model = m3,
          col = clean_data$Survived + 5)
```

### Diagnostic Plots
```{r}
plot(x = base_mod,
     which = 1:6,
     col = data$Survived + 5)
```

